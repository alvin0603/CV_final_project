import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import transforms
from pathlib import Path
import pandas as pd
import numpy as np
import os
from sklearn.metrics import roc_auc_score, average_precision_score

from dataset import MVTecDataset, load_mvtec_data
from model import MVTecResNet

# Configuration
IMG_ROOT = Path("/home/choulin/CV_final_project/data/MVTec")
BATCH_SIZE = 32

def compute_metrics(labels, probs):
    if len(np.unique(labels)) < 2:
        return 0.0, 0.5
    try:
        auc = roc_auc_score(labels, probs)
        ap = average_precision_score(labels, probs)
    except:
        auc = 0.5
        ap = 0.0
    return ap, auc

def main():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")
    
    # Load data - WE USE ALL DATA for evaluation, but segment by category
    # Actually, we should probably evaluate on the 'test' split portion or the 'val' split?
    # The requirement is "output metrics similar to CSV". The CSV lists all categories.
    # To be consistent with the training process (where we merged everything), 
    # we should probably just use the Validation Set generated by our random split 
    # OR we can evaluate on the dedicated 'test' folders if we want to mimic standard prod.
    # BUT, since we trained on a mix of everything, the standard 'test' set might be contaminated (exist in training).
    
    # DECISION: We will use the VAL set from our split logic to ensure no data leakage.
    # We will filter the VAL set by category to compute per-category metrics.
    
    _, val_df = load_mvtec_data(IMG_ROOT, random_state=42)
    print(f"Evaluation locally on random split Validation set ({len(val_df)} images)")

    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),    
    ])
    
    # Preparing model
    model = MVTecResNet(pretrained=False)
    # Load best model relative to script location
    model_path = Path(__file__).parent / "best_model.pth"
    try:
        model.load_state_dict(torch.load(model_path, map_location=device))
        print(f"Loaded {model_path}")
    except FileNotFoundError:
        print(f"Model not found at {model_path}, cannot evaluate.")
        return

    model = model.to(device)
    model.eval()
    
    # We run inference on the WHOLE validation set first for efficiency
    val_ds = MVTecDataset(val_df, transform=val_transform)
    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)
    
    all_probs = []
    all_labels = []
    all_categories = []
    
    with torch.no_grad():
        for i, (imgs, labels) in enumerate(val_loader):
            imgs = imgs.to(device)
            
            outputs = model(imgs)
            probs = torch.sigmoid(outputs).cpu().numpy().flatten()
            
            all_probs.extend(probs)
            all_labels.extend(labels.numpy().flatten())
            
            # We need to map back to calculate per-dict
            # The dataloader shuffles? No, shuffle=False.
            # But we need category info. MVTecDataset dataframe has it.
            # We can just iterate the dataframe if the order is preserved.
    
    # MVTecDataset preserves order of dataframe passed to it.
    # So we can just assign the predictions back to the dataframe.
    val_df['prob'] = all_probs
    val_df['gt_label'] = all_labels # Just to confirm
    
    # Now compute metrics per category
    results_list = []
    categories = sorted(val_df['category'].unique())
    
    print("\nComputing per-category metrics...")
    
    for cat in categories:
        cat_df = val_df[val_df['category'] == cat]
        
        y_true = cat_df['gt_label'].values
        y_scores = cat_df['prob'].values
        
        ap, auc = compute_metrics(y_true, y_scores)
        
        # Max Confidences
        # Anomaly = Label 1, Normal = Label 0
        anom_scores = y_scores[y_true == 1]
        norm_scores = y_scores[y_true == 0]
        
        max_conf_anom = anom_scores.mean() if len(anom_scores) > 0 else 0.0
        max_conf_norm = norm_scores.mean() if len(norm_scores) > 0 else 0.0
        
        print(f"Category: {cat:<15} | AUC: {auc:.4f} | AP: {ap:.4f} | Mean Conf Anom: {max_conf_anom:.4f}")
        
        results_list.append({
            "Category": cat,
            "Image_AP": f"{ap:.4f}",
            "Image_AUC": f"{auc:.4f}",
            "Max_Conf_Anomaly": f"{max_conf_anom:.4f}",
            "Max_Conf_Normal": f"{max_conf_norm:.4f}"
        })

    # Add Average Row? The example csv didn't explicitly show it but usually good.
    # User's example had 17 lines, 15 categories + header + empty line maybe?
    # Let's just output the categories.
    
    # Save to CSV
    # Save relative to script location
    output_dir = Path(__file__).parent
    output_csv = output_dir / "result.csv"
    
    res_df = pd.DataFrame(results_list)
    res_df.to_csv(output_csv, index=False)
    print(f"\nSaved results to {output_csv}")

if __name__ == "__main__":
    main()
